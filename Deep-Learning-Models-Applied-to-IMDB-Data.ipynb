{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep learning models for sentiment analysis.\n\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHcPmimTJL2f4KBVmtG3QPYjHgVVGDoywevA&usqp=CAU\" width=\"333\" class=\"center\"/>\n\nThis notebook aims to show how it is possible to solve the problem of classification into positive and negative feelings. I'm going to start by presenting the data that we're going to study. Then explain the principles of tokenization. Then I will present four models using deep learning methods. To finally use these four models in a single prediction. The  models will be created with pytorch.\n\n\n\n### Table of Contents\n\n* <a href=\"#Presentation of data.\">Presentation of data.</a>\n* <a href=\"#GLOVE Pretrained Emebedding\">GLOVE Pretrained Emebedding</a>\n* <a href=\"#LSTM Clasifier\">LSTM Clasifier</a>\n* <a href=\"#Fine tuning BERT\">Fine tuning BERT</a>\n* <a href=\"#Fine tuning XLNET\">Fine tuning XLNET</a>\n* <a href=\"#Model comparison.\">Model comparison.</a>\n\n### Indications before running the notebook.\n\n* You will only be able to run this notebook if you have a GPU, so if your computer does not have one you can go to Google Colaboratory to get access to a GPU. In the code below I assume that you have your notebook on google colaboratory and that you have modified the type of execution to activate a GPU. If so, you should see \"Device: cuda\" as a message.\n* Also I assume you have downloaded imdb dataset locally. If this is the case you will need to modify ```path_data``` so that it indicates the position of the data.\n\n","metadata":{"id":"uyDFgEpU0Z7l"}},{"cell_type":"code","source":"colab=False\nif colab: \n    from google.colab import drive\n    import torch \n\n    drive.mount('/content/drive')\n    path_data=\"/content/drive/My Drive/IPP-M2-DS/Deep with python/Project IMDB/Data/IMDB Dataset.csv\"\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"Device :\",device)","metadata":{"id":"x5TMjAQN8Jnp","outputId":"8020c49e-c5ca-425c-df7b-0529350eda9d","execution":{"iopub.status.busy":"2022-03-31T04:50:32.309489Z","iopub.execute_input":"2022-03-31T04:50:32.30974Z","iopub.status.idle":"2022-03-31T04:50:32.315184Z","shell.execute_reply.started":"2022-03-31T04:50:32.309712Z","shell.execute_reply":"2022-03-31T04:50:32.314209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"concerning the packages you will have to install two packages on google colaboratory with the following commands:","metadata":{"id":"liTg271B68Zw"}},{"cell_type":"code","source":"#!pip install transformers\n#!pip install datasets","metadata":{"id":"gpP21JV30WxT","execution":{"iopub.status.busy":"2022-03-31T04:50:35.064741Z","iopub.execute_input":"2022-03-31T04:50:35.06557Z","iopub.status.idle":"2022-03-31T04:50:35.068918Z","shell.execute_reply.started":"2022-03-31T04:50:35.065521Z","shell.execute_reply":"2022-03-31T04:50:35.068213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm, trange\nimport random\nimport time\nimport pickle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport pandas as pd\nimport math\nimport time\nimport pprint\npp = pprint.PrettyPrinter()\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.optim as optim\nfrom torchtext.legacy import data\nfrom torchtext.data.utils import get_tokenizer\n\nimport torch.nn as nn\n\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, get_scheduler\nfrom transformers import XLNetConfig,AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\nfrom datasets import Dataset, load_dataset, load_metric\n\nfrom keras.preprocessing.sequence import pad_sequences\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","metadata":{"id":"D-cjAqIaUKvK","execution":{"iopub.status.busy":"2022-03-31T12:04:28.993143Z","iopub.execute_input":"2022-03-31T12:04:28.993443Z","iopub.status.idle":"2022-03-31T12:04:36.242439Z","shell.execute_reply.started":"2022-03-31T12:04:28.993367Z","shell.execute_reply":"2022-03-31T12:04:36.241658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results dictionary will allow you to store the results as the experiments progress.\n","metadata":{"id":"K9pCjHSsWsal"}},{"cell_type":"code","source":"results={}\nmodels_name = [\"BERT\",\"XLNET\",\"LSTM\",\"GLOVE\"]\nfor model_name in models_name:\n  results[model_name] = {\"Train Loss\":[],\"Train Acc\":[],\"Val. Loss\": [],\"Val. Acc\":[],\"test. Acc\":0,\"test. loss\":0,\"Training time\":0}\n\nEPOCH_NUMBER = 12","metadata":{"id":"dwLoSkinWrFx","execution":{"iopub.status.busy":"2022-03-31T12:04:36.245079Z","iopub.execute_input":"2022-03-31T12:04:36.245509Z","iopub.status.idle":"2022-03-31T12:04:36.250859Z","shell.execute_reply.started":"2022-03-31T12:04:36.245471Z","shell.execute_reply":"2022-03-31T12:04:36.250205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Functions\"></a>\n# Functions\n\nDuring this notebook we will use 3 main functions. One for model training (```train```). One for validation (```evaluate```) and finally one to visualize the performance (```plot_results```) of our models. Note that the train and validation function takes the model you want to train as an argument. We do this because the 4 models do not have exactly the same types of arguments for training and they do not use the same dataloader.\n","metadata":{"id":"h-0r03LYz-6k"}},{"cell_type":"code","source":"def train(model,iterator,optimizer,criterion, train_glove=False,train_LSTM=False,train_pretrain=False):\n  torch.cuda.empty_cache()\n  epoch_loss = 0.0\n  epoch_acc = 0.0\n  \n  model.train()\n  metric = load_metric(\"accuracy\")\n  for batch in iterator:\n      optimizer.zero_grad()\n\n      if train_LSTM:\n\n        text,text_lengths = batch.review\n        predictions = model(text,text_lengths).squeeze()\n        loss = criterion(predictions,batch.sentiment)\n\n        correct = (torch.round(predictions) == batch.sentiment).float() \n        acc = correct.sum() / len(correct)\n\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        loss.backward()\n        optimizer.step()\n\n\n      if train_glove:\n\n        text,text_lengths = batch.review\n        predictions = model(text).squeeze()\n        loss = criterion(predictions,batch.sentiment)\n\n        correct = (torch.round(predictions) == batch.sentiment).float() \n        acc = correct.sum() / len(correct)\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        loss.backward()\n        optimizer.step()\n\n\n      if train_pretrain:\n        \n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        predictions = outputs.logits\n        predictions = torch.argmax(predictions, dim=-1)\n        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n        epoch_loss += loss\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n  if  train_pretrain:\n     return epoch_loss / len(iterator), metric.compute()[\"accuracy\"]\n\n  if not train_pretrain:\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model,iterator,criterion, train_glove=False,train_LSTM=False,train_pretrain=False):\n\n    epoch_loss = 0.0\n    epoch_acc = 0.0\n    \n    # deactivate the dropouts\n    model.eval()\n    metric = load_metric(\"accuracy\")\n    # Sets require_grad flat False\n    with torch.no_grad():\n        for batch in iterator:\n            if train_LSTM:\n\n              text,text_lengths = batch.review\n              predictions = model(text,text_lengths).squeeze()\n              loss = criterion(predictions,batch.sentiment)\n\n              correct = (torch.round(predictions) == batch.sentiment).float() \n              acc = correct.sum() / len(correct)\n\n              epoch_loss += loss.item()\n              epoch_acc += acc.item()\n\n            if train_glove:\n\n              text,text_lengths = batch.review\n              predictions = model(text).squeeze()\n              loss = criterion(predictions,batch.sentiment)\n\n              correct = (torch.round(predictions) == batch.sentiment).float() \n              acc = correct.sum() / len(correct)\n\n              epoch_loss += loss.item()\n              epoch_acc += acc.item()\n\n            if train_pretrain:\n              \n              batch = {k: v.to(device) for k, v in batch.items()}\n              outputs = model(**batch)\n              loss = outputs.loss\n              predictions = outputs.logits\n              \n              predictions = torch.argmax(predictions, dim=-1)\n\n              lr_scheduler.step()\n              metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\n\n\n\n              epoch_loss += loss\n    if  train_pretrain:\n      return epoch_loss / len(iterator), metric.compute()[\"accuracy\"]\n    if not train_pretrain:\n      return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef plot_results(model_name):\n  plt.style.use(\"seaborn\")\n  fig, (ax1,ax2) = plt.subplots(2,sharex=True)\n  ax1.set_title(model_name,fontsize=18,fontstyle='italic')\n\n  ax1.plot(results[model_name][\"Train Acc\"],label=\"Train Acc\",linewidth=3)\n  ax1.plot(results[model_name][\"Val. Acc\"],label=\"Val. Acc\",linewidth=3)\n  ax1.legend(loc='upper left', shadow=True)\n  ax1.set_xlabel('EPOCHS')\n  ax1.set_ylabel('Accuracy')\n\n  ax2.plot(results[model_name][\"Train Loss\"],label=\"Train Loss\",linewidth=3)\n  ax2.plot(results[model_name][\"Val. Loss\"],label=\"Val. Loss\",linewidth=3)\n  ax2.legend(loc='upper left', shadow=True)\n  ax2.set_xlabel('EPOCHS')\n  ax2.set_ylabel('Loss')\n\n  fig.set_figheight(12.3)\n  fig.set_figwidth(10)","metadata":{"id":"qNF2QE6QBIjU","execution":{"iopub.status.busy":"2022-03-31T12:04:41.587923Z","iopub.execute_input":"2022-03-31T12:04:41.588343Z","iopub.status.idle":"2022-03-31T12:04:41.613143Z","shell.execute_reply.started":"2022-03-31T12:04:41.588305Z","shell.execute_reply":"2022-03-31T12:04:41.611911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Presentation of data.\n","metadata":{"id":"LWpUCDAkuvtE"}},{"cell_type":"code","source":"path_data=\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:04:44.993097Z","iopub.execute_input":"2022-03-31T12:04:44.993546Z","iopub.status.idle":"2022-03-31T12:04:44.998219Z","shell.execute_reply.started":"2022-03-31T12:04:44.99351Z","shell.execute_reply":"2022-03-31T12:04:44.997398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path_data)\ndf.head()","metadata":{"id":"SB7QTz1Ku3NB","outputId":"5da9633c-9f94-45bd-eb6c-308e6c3b33db","execution":{"iopub.status.busy":"2022-03-31T06:42:51.452303Z","iopub.execute_input":"2022-03-31T06:42:51.452715Z","iopub.status.idle":"2022-03-31T06:42:52.369594Z","shell.execute_reply.started":"2022-03-31T06:42:51.452669Z","shell.execute_reply":"2022-03-31T06:42:52.368608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.pprint(df.loc[0][\"review\"])\nprint(\"Sentiment :\",df.loc[0][\"sentiment\"])","metadata":{"id":"dwhAvrOuGb8G","outputId":"7b1adb01-af9a-4d11-9fdf-13b80c4cb37d","execution":{"iopub.status.busy":"2022-03-31T06:42:52.374506Z","iopub.execute_input":"2022-03-31T06:42:52.376559Z","iopub.status.idle":"2022-03-31T06:42:52.394535Z","shell.execute_reply.started":"2022-03-31T06:42:52.376509Z","shell.execute_reply":"2022-03-31T06:42:52.388998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the same number of positive and negative reviews, a balanced dataset. We can use the accuracy to compare our results.\n","metadata":{"id":"u5xPGE3YGOF1"}},{"cell_type":"code","source":"df[\"sentiment\"].value_counts()","metadata":{"id":"wM5OFhtaFhrV","outputId":"c3edc10e-3967-45b1-961f-015ca45199de","execution":{"iopub.status.busy":"2022-03-31T06:42:52.399156Z","iopub.execute_input":"2022-03-31T06:42:52.399437Z","iopub.status.idle":"2022-03-31T06:42:52.423809Z","shell.execute_reply.started":"2022-03-31T06:42:52.399393Z","shell.execute_reply":"2022-03-31T06:42:52.422468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"id":"gwg6elctHp65","outputId":"7a1205c2-f9f5-4259-bdcc-6e173f2cfe10","execution":{"iopub.status.busy":"2022-03-31T06:42:52.425469Z","iopub.execute_input":"2022-03-31T06:42:52.425972Z","iopub.status.idle":"2022-03-31T06:42:52.439635Z","shell.execute_reply.started":"2022-03-31T06:42:52.425926Z","shell.execute_reply":"2022-03-31T06:42:52.438175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the graph below we see the distribution of the number of tokens if we had created a token for each space. This gives an idea of ​​the size of the texts. We see that the size is far from homogeneous. We see for example that the distribution is concentrated around 250 tokens but that we have documents with more than 2000 tokens.\n\n","metadata":{}},{"cell_type":"code","source":"plt.title('Number of tokens per document')\nsns.displot(df[\"review\"].apply(lambda x : len(x.split(' '))).tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GLOVE PRETRAINED EMBEDDING\n\n[GLOVE](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) is a word embedding that was created in 2014 by researchers from Stanford Univeristy. The idea of ​​word embedding is to capture the semantics of a word inside a vector. So two close words in the language are expected to have a close embedding as well. As explained in their [paper](https://nlp.stanford.edu/pubs/glove.pdf). An embedding is expected to capture complex relationships, such as the distance between the male and female vectors being the same as the distance between the king and queen vectors: vec(king) −\nvec(queen) = vec(man) − vec(woman). As we can see below glove is able to create the embedding of 400000 words, for vectors of size 300. Like wor2vec GLOVE is a log-bilinear language model. It is trained to predict the next word using previous words. But instead of using the co-occurence matrix as is the case for word2vec, it will use an improved version of this matrix. The idea is to use a ratio of these co-occurence probability, instead of just co-occurences.\n\n\n\n\n\n\n","metadata":{"id":"BUbtE-6N229R"}},{"cell_type":"code","source":"def get_glove_adapted_embeddings(glove_model, input_voc):\n    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n    embeddings = np.zeros((len(input_voc)+1,glove_model.vectors.shape[1]))\n    for i, ind in index_dict.items():\n        embeddings[i] = glove_model.vectors[ind]\n    return embeddings\n\nimport gensim.downloader as api\n# loaded_glove_model = api.load(\"glove-wiki-gigaword-300\") better\nloaded_glove_model = api.load(\"glove-twitter-25\")\nprint(\"GLOVE size : \",loaded_glove_model.vectors.shape)","metadata":{"id":"DN6-Evh0QsZm","outputId":"3283d924-4708-48c8-849b-8d0e5656b66c","execution":{"iopub.status.busy":"2022-03-31T06:42:52.441257Z","iopub.execute_input":"2022-03-31T06:42:52.441703Z","iopub.status.idle":"2022-03-31T06:44:12.076651Z","shell.execute_reply.started":"2022-03-31T06:42:52.441658Z","shell.execute_reply":"2022-03-31T06:44:12.075506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see in the example below how glove works. Here I first recovered the vectors of size 300 then I applied a pca to keep only two main axes (of maximum variance). We can clearly see that the distance between the masculine word and its feminine equivalent are almost always at the same distance.\n","metadata":{"id":"Brv-nsYSVrS1"}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"def get_glove_adapted_embeddings(glove_model, training_word2idx):\n    \n    keys ={}\n    for w,i in training_word2idx.items():\n        try:\n            keys[i]=glove_model.word_vec(w, None)\n        except:\n            pass\n    #keys = {i: glove_model.word_vec(w, None) for w, i in training_word2idx.items()}\n    index_dict = {i: key for i, key in keys.items() if key is not None}\n    embeddings = np.zeros((len(training_word2idx)+1,glove_model.vectors.shape[1]))\n    for i, ind in training_word2idx.items():\n        embeddings[ind] = glove_model.vectors[ind]\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2022-03-31T07:01:47.285396Z","iopub.execute_input":"2022-03-31T07:01:47.286495Z","iopub.status.idle":"2022-03-31T07:01:47.295609Z","shell.execute_reply.started":"2022-03-31T07:01:47.286444Z","shell.execute_reply":"2022-03-31T07:01:47.293919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words=[\"king\",\"queen\",\"sister\",\"brother\"]\npp.pprint(words)\ntraining_word2idx = dict(zip(words,range(len(words))))\nGloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, training_word2idx)\n\npca = PCA(n_components=2)\nreduce_embedding = pca.fit_transform(GloveEmbeddings[range(len(words))])\n\nplt.style.use('seaborn')\nfig, ax = plt.subplots()\nax.scatter(reduce_embedding[:,0], reduce_embedding[:,1])\nfor i, txt in enumerate(training_word2idx.keys()):\n    ax.annotate(txt, (reduce_embedding[:,0][i], reduce_embedding[:,1][i]))","metadata":{"id":"Wh6kZLqeT4x7","outputId":"896870f1-683d-4b60-8a63-4f3f9adb56f7","execution":{"iopub.status.busy":"2022-03-31T07:07:28.089468Z","iopub.execute_input":"2022-03-31T07:07:28.08976Z","iopub.status.idle":"2022-03-31T07:07:28.411743Z","shell.execute_reply.started":"2022-03-31T07:07:28.089727Z","shell.execute_reply":"2022-03-31T07:07:28.410824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of ​​a tokenizer is to take a text as input and give us the associated tokens. The choice of tokens varies according to the models. Generally, we have a token when we have a space in a sentence. Thus a word corresponds to a token just like a punctuation. However, this definition suffers from many problems, such as compound words. Air France does not have the same meaning as Air and France. During this notebook we will use popular tokenizers in order to guarantee the quality of the tokens created. In the example below, I import the spacy tokenizer and I give an example of what we get by applying it to the first document of the IMDB dataset.\n\n","metadata":{"id":"B3jCJpN0d-ym"}},{"cell_type":"code","source":"tokenizer = get_tokenizer('spacy')\npp.pprint(tokenizer(df.loc[0][\"review\"])[:18])","metadata":{"id":"itSBcCSQW1_g","outputId":"1f0d2b27-b404-490f-969c-9f179b29eec0","execution":{"iopub.status.busy":"2022-03-31T06:54:17.870008Z","iopub.execute_input":"2022-03-31T06:54:17.870334Z","iopub.status.idle":"2022-03-31T06:54:24.268686Z","shell.execute_reply.started":"2022-03-31T06:54:17.870301Z","shell.execute_reply":"2022-03-31T06:54:24.267665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To simplify learning we will use dataloaders for training. A train dataloader, test and validate. In order to compare the models we will always use 25000 data to test our model, 20000 to train it and 5000 to validate.\n","metadata":{"id":"57Fl7-XAg0s8"}},{"cell_type":"code","source":"TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True)\nfields = [('review',TEXT),(\"sentiment\",LABEL)]\ntraining_data = data.TabularDataset(path=path_data,\n                                    format=\"csv\",\n                                    fields=fields,\n                                    skip_header=True\n                                   )\n\ntrain_data,test_data = training_data.split(split_ratio=0.5)\ntrain_data,valid_data = train_data.split(split_ratio=0.8)\nTEXT.build_vocab(train_data,min_freq=5)\nLABEL.build_vocab(train_data)\nBATCH_SIZE = 32\ntrain_iterator = data.BucketIterator(\n        dataset=train_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\n\ntest_iterator = data.BucketIterator(\n        dataset=test_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\n\nvalidation_iterator = data.BucketIterator(\n        dataset=valid_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\nprint(\"Train size = {}, Test size = {}, valid size = {}\".format(len(train_data),len(test_data),len(valid_data)))","metadata":{"id":"kOsEYCDwylo2","outputId":"ba25e55a-36cc-42da-89d0-e456315b2a7a","execution":{"iopub.status.busy":"2022-03-31T06:54:26.556283Z","iopub.execute_input":"2022-03-31T06:54:26.55699Z","iopub.status.idle":"2022-03-31T06:55:38.198072Z","shell.execute_reply.started":"2022-03-31T06:54:26.556953Z","shell.execute_reply":"2022-03-31T06:55:38.195918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our model, we will take the GLOVE embedding that we will fine tune to adapt it to our data. To do so, we specify requires_grad=True. Then we will average the embeddings for our document, which will allow us to obtain a vector of size 300 for each document. Then we will simply use this vector of size 300 as input to a linear model and end with a sigmoid activation to have a probability between 0 and 1 as output.\n","metadata":{"id":"xqbC5fbQiiog"}},{"cell_type":"code","source":"class PretrainedAveragingModel(nn.Module):\n\n    def __init__(self, embedding_dim, vocabulary_size,pretrained_embeddings,fine_tuning):\n      super().__init__()\n      self.embeddings = nn.Embedding(num_embeddings=vocabulary_size+1,embedding_dim=embedding_dim)\n      self.embeddings.from_pretrained(pretrained_embeddings)\n      self.embeddings.weight.requires_grad=fine_tuning\n      self.linear = nn.Linear(in_features=embedding_dim,out_features=1) \n      self.act = nn.Sigmoid()\n      \n    def forward(self, inputs):\n        x =  torch.mean(self.embeddings(inputs),1) \n        o = self.linear(x)\n        o = self.act(o)\n        return o","metadata":{"id":"uaf311Dc5HWP","execution":{"iopub.status.busy":"2022-03-31T06:55:59.526617Z","iopub.execute_input":"2022-03-31T06:55:59.527177Z","iopub.status.idle":"2022-03-31T06:55:59.539257Z","shell.execute_reply.started":"2022-03-31T06:55:59.527128Z","shell.execute_reply":"2022-03-31T06:55:59.53826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How we are on a binary classification problem we will use the binary cross entropy as loss. For the optimization we use ADAM.\n","metadata":{"id":"9rXBKVnMr5WQ"}},{"cell_type":"code","source":"training_word2idx = TEXT.vocab.stoi\nGloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, training_word2idx)\ncriterion = nn.BCELoss().to(device)\nmodel_pre_trained = PretrainedAveragingModel(300, len(training_word2idx), torch.FloatTensor(GloveEmbeddings), True).to(device)\nopt_pre_trained = optim.Adam(model_pre_trained.parameters(), lr=0.0025, betas=(0.9, 0.999))","metadata":{"id":"hqDQPJae-FXA","execution":{"iopub.status.busy":"2022-03-31T07:01:52.400159Z","iopub.execute_input":"2022-03-31T07:01:52.400476Z","iopub.status.idle":"2022-03-31T07:01:54.872706Z","shell.execute_reply.started":"2022-03-31T07:01:52.400442Z","shell.execute_reply":"2022-03-31T07:01:54.871745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During the training of each model, we will save the results in the results dictionary. For each epoch we will train on the dataloader train and validate on the dataloader val. Then when the training is finished we will test our results on test dataloader. Time is also saved to compare this aspect also of the different models.\n","metadata":{"id":"KA8YjSpmsMj0"}},{"cell_type":"code","source":"EPOCH_NUMBER=8\nstart = time.time()\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc = train(model_pre_trained,train_iterator,opt_pre_trained,criterion,train_glove=True)\n    valid_loss,valid_acc = evaluate(model_pre_trained,validation_iterator,criterion,train_glove=True)\n    \n    results[\"GLOVE\"][\"Train Loss\"].append(train_loss)\n    results[\"GLOVE\"][\"Train Acc\"].append(train_acc*100)\n    results[\"GLOVE\"][\"Val. Loss\"].append(valid_loss)\n    results[\"GLOVE\"][\"Val. Acc\"].append(valid_acc*100)\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print()\n\ntest_loss,test_acc = evaluate(model_pre_trained,test_iterator,criterion,train_glove=True)\nprint(f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc*100:.2f}%')\nstop = time.time()\nprint(f\"\\t Training time: {stop - start}s\")\nresults[\"GLOVE\"][\"Training time\"] = stop - start\nresults[\"GLOVE\"][\"test. loss\"] = test_loss\nresults[\"GLOVE\"][\"test. Acc\"] = test_acc","metadata":{"id":"_z_4HEMuj3HR","outputId":"f6fcff94-fcdb-4463-ab28-dca670936677","execution":{"iopub.status.busy":"2022-03-31T06:59:35.664126Z","iopub.execute_input":"2022-03-31T06:59:35.664458Z","iopub.status.idle":"2022-03-31T07:00:48.843526Z","shell.execute_reply.started":"2022-03-31T06:59:35.664425Z","shell.execute_reply":"2022-03-31T07:00:48.8425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(\"GLOVE\")","metadata":{"id":"YKuOJv-xawDB","outputId":"953311e8-8081-4ec9-98c3-817d910d358a","execution":{"iopub.status.busy":"2022-03-31T07:00:53.226327Z","iopub.execute_input":"2022-03-31T07:00:53.227125Z","iopub.status.idle":"2022-03-31T07:00:53.756637Z","shell.execute_reply.started":"2022-03-31T07:00:53.227091Z","shell.execute_reply":"2022-03-31T07:00:53.755572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After 145 seconds of training we obtain an accuracy on the 25000 data (test sample) of 88% which is quite good.\n","metadata":{"id":"JurHbMUstLdq"}},{"cell_type":"markdown","source":"# LSTM CLASSIFIER","metadata":{"id":"IN8snBXM47NU"}},{"cell_type":"markdown","source":"Unlike the previous model here, we will start from a randomly initialized embedding that we will train. We will also use a bidirectional LSTM. The second layer use the pack_padded_sequence function. without going into too much detail, the idea of ​​this function is to save calculation time but will not influence the result. As you can see this model uses an LSTM layer.\n\n\n With pytorch the lstm layer returns a lot of information to us, it is not always easy to see clearly which one to use. As a reminder, one of the problems with RNNs comes from gradient backpropagation. When the chain is too large, the gradient tends either to explode or to disappear, which makes learning long-term relationships very complicated for this type of model. Hence the interest of using an LSTM. The LSTM will solve this problem by having a long-term memory. Thus LSTM receives three sources of information at the input of each cell, the data, the short-term memory which is recorded in the hidden states and the long-term memory which is recorded in the cell states. The LSTM cell will also return three items.\n\n* output : which is composed of all the hidden states in the last layer.\n* hidden : hidden state which contains information about the previous time step.\n* Cell : cell which contains information about all previous information in the sentence.\n\n\n\nWe will also use a bidirectional LSTM. The idea is that we will pass the data in one direction but also in the other. Thus we will use the information contained in the past and the future to predict the words. By having a bidirectional LSTM we will therefore have two hidden states. One for the forward part and the other for the backward part of the LSTM (nothing to do with the gradient here). We will retrieve the last hidden states from the forward and backward part that we will concatenate before using it as features in a fully connected layer.\n\n\n\n\n\n\n\n","metadata":{"id":"np2JOTzatnbp"}},{"cell_type":"code","source":"\nclass classifier(nn.Module):\n    \n    #define all the layers used in model\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout):\n        super().__init__()          \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout,\n                           batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text, text_lengths):\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True,enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n        dense_outputs=self.fc(hidden)\n        outputs=self.act(dense_outputs)\n        return outputs","metadata":{"id":"PPhgeNUyuhFd","execution":{"iopub.status.busy":"2022-03-31T07:02:06.228131Z","iopub.execute_input":"2022-03-31T07:02:06.228453Z","iopub.status.idle":"2022-03-31T07:02:06.238981Z","shell.execute_reply.started":"2022-03-31T07:02:06.22842Z","shell.execute_reply":"2022-03-31T07:02:06.237612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = classifier (vocab_size=len(TEXT.vocab),\n                    embedding_dim=100,\n                    hidden_dim=63,\n                    output_dim=1,\n                    n_layers=2,\n                    bidirectional=True,\n                    dropout=0.2).to(device)\ncriterion = nn.BCELoss().to(device)\noptimizer = optim.Adam(model.parameters(),lr=1e-4)","metadata":{"id":"8hW4F-_vxB99","execution":{"iopub.status.busy":"2022-03-31T07:02:09.784042Z","iopub.execute_input":"2022-03-31T07:02:09.784737Z","iopub.status.idle":"2022-03-31T07:02:10.692941Z","shell.execute_reply.started":"2022-03-31T07:02:09.784698Z","shell.execute_reply":"2022-03-31T07:02:10.691929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start =time.time()\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc = train(model,train_iterator,optimizer,criterion,train_LSTM=True)\n    valid_loss,valid_acc  = evaluate(model,validation_iterator,criterion,train_LSTM=True)\n  \n    results[\"LSTM\"][\"Train Loss\"].append(train_loss)\n    results[\"LSTM\"][\"Train Acc\"].append(train_acc*100)\n    results[\"LSTM\"][\"Val. Loss\"].append(valid_loss)\n    results[\"LSTM\"][\"Val. Acc\"].append(valid_acc*100)\n    # Showing statistics\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print()\n\ntest_loss,test_acc = evaluate(model,test_iterator,criterion,train_LSTM=True)\nprint(f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc*100:.2f}%')\n\nstop = time.time()\nprint(f\"\\t Training time: {stop - start}s\")\nresults[\"LSTM\"][\"Training time\"] = stop - start\nresults[\"LSTM\"][\"test. loss\"] = test_loss\nresults[\"LSTM\"][\"test. Acc\"] = test_acc","metadata":{"id":"R2kFIijHxmNU","outputId":"160d2089-fdd9-4ab2-8d63-a76fa6d724ac","execution":{"iopub.status.busy":"2022-03-31T07:08:26.630759Z","iopub.execute_input":"2022-03-31T07:08:26.631055Z","iopub.status.idle":"2022-03-31T07:25:54.826844Z","shell.execute_reply.started":"2022-03-31T07:08:26.631022Z","shell.execute_reply":"2022-03-31T07:25:54.825819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(\"LSTM\")","metadata":{"id":"_6fCQy_1fr0i","outputId":"893a5ce6-12d8-4e67-bc07-7f407787db8e","execution":{"iopub.status.busy":"2022-03-31T07:25:54.829261Z","iopub.execute_input":"2022-03-31T07:25:54.829995Z","iopub.status.idle":"2022-03-31T07:25:55.323632Z","shell.execute_reply.started":"2022-03-31T07:25:54.82995Z","shell.execute_reply":"2022-03-31T07:25:55.32267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the use of an LSTM layer does not necessarily improve our model if we compare it to GLOVE. There is also a very significant increase in computation times.\n","metadata":{"id":"fg1IHGkP9jl1"}},{"cell_type":"markdown","source":"# Fine tuning\n\nAs we had fine tune GLOVE to fit our data. We will now fine tune the models. We are going to look at two more complex models now and see if their use really makes sense in the case of a dataset like IMDB Dataset. During this part on fine tuning we will focus on two well-known models [BERT](https://arxiv.org/pdf/1810.04805.pdf) and [XLNET](https://arxiv.org/pdf/1906.08237.pdf). To fine tune these models we will use the very practical \"transformers\" library of [hugging face](https://huggingface.co/). \n\n\n","metadata":{"id":"G6zC4zDkvUqp"}},{"cell_type":"markdown","source":"I'll start by quickly explaining what a transformer is. Transformers were proposed in 2017 in this [paper](https://arxiv.org/pdf/1706.03762.pdf), and have since been state-of-the-art in NLP. They are the state of the art in all the tasks of NLP which was until then mainly dominated by LSTMs and RNNs. In addition, the great progress of these models is that it will take the entire sentence as input and no longer sequentially as before. This will greatly improve computation times. \n\n\n","metadata":{"id":"RUJVpV6qDGsz"}},{"cell_type":"code","source":"model_max_length=128\nbatch_size=32\nEPOCH_NUMBER=8","metadata":{"id":"hOmVR7yrve90","execution":{"iopub.status.busy":"2022-03-31T12:07:20.211193Z","iopub.execute_input":"2022-03-31T12:07:20.211942Z","iopub.status.idle":"2022-03-31T12:07:20.215958Z","shell.execute_reply.started":"2022-03-31T12:07:20.21188Z","shell.execute_reply":"2022-03-31T12:07:20.215141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLNET\n\nIn the family of transformers there are a large number of different architectures. One of the most popular is XLNET. The idea of ​​this method is to calculate the probability of a word based on all the permutations of all the other words in the sentence. Here we don't use the right and left context as an LSTM but we use permutations of words as context to predict a word.\n\n\nAs I use the transformer library for this fine tuning part. I have to re-create my dataloaders. I will therefore use the same distribution train, test, valid.\n","metadata":{"id":"12v0tV2DlWkh"}},{"cell_type":"code","source":"raw_datasets = load_dataset('csv', data_files=path_data)\nraw_datasets[\"train\"]= raw_datasets[\"train\"].add_column(\"labels\", pd.Series(raw_datasets[\"train\"][\"sentiment\"]).map({'negative':0,'positive':1}).tolist())\nraw_datasets=raw_datasets[\"train\"].train_test_split(0.5)\nraw_data_valid = raw_datasets[\"train\"].train_test_split(0.2)\nraw_datasets[\"train\"]= raw_data_valid[\"train\"]\nraw_datasets[\"valid\"]= raw_data_valid[\"test\"]\nraw_datasets","metadata":{"id":"p4A7ao-WPNOA","outputId":"ee0fb09c-ae66-43c1-cec0-b6c64992f760","execution":{"iopub.status.busy":"2022-03-31T12:07:54.268784Z","iopub.execute_input":"2022-03-31T12:07:54.269319Z","iopub.status.idle":"2022-03-31T12:07:56.57155Z","shell.execute_reply.started":"2022-03-31T12:07:54.269281Z","shell.execute_reply":"2022-03-31T12:07:56.570751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also the tokenizer that must be used is the one associated with XLNET. Indeed, as we are going to use an already trained model, it must take as input the same type of tokens, but also the same coding of a word to a number. For this we use AutoTokenizer.from_pretrained which allows to load the tokenizer.\n","metadata":{"id":"DGMR0s5xIgJQ"}},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"review\"], truncation=True)\n\ntokenizer  = AutoTokenizer.from_pretrained(\"xlnet-base-cased\",model_max_length=model_max_length)\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ntokenized_datasets = tokenized_datasets.remove_columns([\"review\",\"sentiment\"])\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"id":"5PuwNULMPM_E","outputId":"d5e8a2cc-2d47-409a-bc0f-9083b8afcf18","execution":{"iopub.status.busy":"2022-03-31T12:08:41.289409Z","iopub.execute_input":"2022-03-31T12:08:41.289709Z","iopub.status.idle":"2022-03-31T12:09:39.780426Z","shell.execute_reply.started":"2022-03-31T12:08:41.28968Z","shell.execute_reply":"2022-03-31T12:09:39.779645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the tokenizer sends us several new elements\n* ```input_ids``` : As before, input_ids corresponds to the tokens that we replaced with a number to give it as input to our model.\n* ```token_type_ids``` : Token type ids has an operation directly linked to the training of the XLNET model. During its training we said that XLNET should predict the next words using the permutations of all the other words. That's true, but that's not the only problem this model has to solve. The second task concerns a pair of sentences that we will give him as input and of which he will have to say whether they are in the right order or not. This parameter is used to indicate for the value 0 it is the first sentence the value 1 is the second and the value 2 is the end of sentence token. As here we don't have this type of objective, we are not interested in this vector.\n* ```attention_mask``` : This vector allows to know if my sequence has been received a padding or not. Here as we took sequences of size 128 we have almost none of our sequences that had a padding. As our sequences almost never have any padding, we have a vector with only 1s, if we had the last 10 values ​​that had been padded, we would have had 10 0s at the end of this vector.\n","metadata":{"id":"rXKtNIDrLJSC"}},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"input_ids\"][0]","metadata":{"id":"vNQuTjVfNoOs","outputId":"750cc7a3-8780-4bc0-8b9b-7e7ff66e72e0","execution":{"iopub.status.busy":"2022-03-31T07:27:17.929405Z","iopub.execute_input":"2022-03-31T07:27:17.929895Z","iopub.status.idle":"2022-03-31T07:27:18.56891Z","shell.execute_reply.started":"2022-03-31T07:27:17.929817Z","shell.execute_reply":"2022-03-31T07:27:18.567737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"token_type_ids\"][0]","metadata":{"id":"JBlICt4ZNnCm","outputId":"c98dcdff-56e7-4f5c-dad7-bd988e71ac59","execution":{"iopub.status.busy":"2022-03-31T07:27:18.570692Z","iopub.execute_input":"2022-03-31T07:27:18.571276Z","iopub.status.idle":"2022-03-31T07:27:19.195072Z","shell.execute_reply.started":"2022-03-31T07:27:18.57121Z","shell.execute_reply":"2022-03-31T07:27:19.193925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"attention_mask\"][0]","metadata":{"id":"1Haj-iz_Iklb","outputId":"eec025cd-df71-44c5-bf36-8baf965822d4","execution":{"iopub.status.busy":"2022-03-31T07:27:19.197885Z","iopub.execute_input":"2022-03-31T07:27:19.198227Z","iopub.status.idle":"2022-03-31T07:27:20.53916Z","shell.execute_reply.started":"2022-03-31T07:27:19.198181Z","shell.execute_reply":"2022-03-31T07:27:20.535634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"valid\"], batch_size=batch_size, collate_fn=data_collator\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n)","metadata":{"id":"EB82Z-P1PM3O","execution":{"iopub.status.busy":"2022-03-31T07:27:20.540915Z","iopub.execute_input":"2022-03-31T07:27:20.541471Z","iopub.status.idle":"2022-03-31T07:27:20.551855Z","shell.execute_reply.started":"2022-03-31T07:27:20.541422Z","shell.execute_reply":"2022-03-31T07:27:20.550778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=EPOCH_NUMBER * len(train_dataloader),)","metadata":{"id":"Z_kp05JKYNUv","outputId":"2264791f-6ed3-458a-9295-2e9c3c59f067","execution":{"iopub.status.busy":"2022-03-31T07:27:20.55353Z","iopub.execute_input":"2022-03-31T07:27:20.554053Z","iopub.status.idle":"2022-03-31T07:27:40.191016Z","shell.execute_reply.started":"2022-03-31T07:27:20.554006Z","shell.execute_reply":"2022-03-31T07:27:40.189945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc = train(model,train_dataloader,optimizer,None,train_pretrain=True)\n    valid_loss,valid_acc = evaluate(model,eval_dataloader,None,train_pretrain=True)\n    \n    results[\"XLNET\"][\"Train Loss\"].append(train_loss.cpu().detach().numpy())\n    results[\"XLNET\"][\"Train Acc\"].append(train_acc*100)\n    results[\"XLNET\"][\"Val. Loss\"].append(valid_loss.cpu().detach().numpy())\n    results[\"XLNET\"][\"Val. Acc\"].append(valid_acc*100)\n    # Showing statistics\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print()\n\ntest_loss,test_acc = evaluate(model,test_dataloader,None,train_pretrain=True)\nprint(f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc*100:.2f}%')\nstop = time.time()\nprint(f\"\\t Training time: {stop - start}s\")\nresults[\"XLNET\"][\"Training time\"] = stop - start\nresults[\"XLNET\"][\"test. loss\"] = test_loss\nresults[\"XLNET\"][\"test. Acc\"] = test_acc\n\n","metadata":{"id":"yoJNDbJfWm8K","outputId":"bc211cbd-b1ec-400d-a8d0-ac800991ff03","execution":{"iopub.status.busy":"2022-03-31T07:31:57.716082Z","iopub.execute_input":"2022-03-31T07:31:57.716434Z","iopub.status.idle":"2022-03-31T08:25:27.224772Z","shell.execute_reply.started":"2022-03-31T07:31:57.716401Z","shell.execute_reply":"2022-03-31T08:25:27.222403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(\"XLNET\")","metadata":{"id":"jzgijBATh7_c","outputId":"a281c620-7e70-4618-cea8-0e80a0ce4b32","execution":{"iopub.status.busy":"2022-03-31T08:25:27.227361Z","iopub.execute_input":"2022-03-31T08:25:27.227699Z","iopub.status.idle":"2022-03-31T08:25:27.986216Z","shell.execute_reply.started":"2022-03-31T08:25:27.227644Z","shell.execute_reply":"2022-03-31T08:25:27.985284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are several interesting things; On the one hand we see that we have put too many EPOCHS, for this type of stain 2 epochs would have been enough for our model. We see that we obtain better performances than with GLOVE but we have improved our results very little compared to the time our model took to train..\n","metadata":{"id":"JNREjkI1QK4w"}},{"cell_type":"markdown","source":"# BERT\n\nBERT is probably the most popular transformer. It has long been the most efficient model. His strongest idea was to modify the way of training the model. Like XLNET, it is trained on two tasks. One is for sentence pairs like XLNET. Bert is trained to predict whether the second sentence is the sentence following the first in the original text. This type of training is called next sentence prediction NSP. But what made BERT known is above all the second task on which he trains. The idea is to mask a certain percentage of input tokens and train the model to predict these tokens using all unmasked tokens. in the figure below taken from the original paper. We can actually see these two spots on the figure on the left. On the figure on the right are examples of possible fine-tuning on three different datasets.\n\n\n\n\n\n\n<img src=\"https://miro.medium.com/max/1400/1*bYO5tEcRzdHtjHV_P6-4ig.png\" width=\"800\" class=\"center\"/>\n\n\n\n","metadata":{"id":"-VmOR0kABpvU"}},{"cell_type":"code","source":"tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-cased\",model_max_length=model_max_length)\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ntokenized_datasets = tokenized_datasets.remove_columns([\"review\",\"sentiment\"])\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"id":"Kybir9NULGgp","outputId":"fbf9e770-3a33-4b20-b493-288fa2801620","execution":{"iopub.status.busy":"2022-03-31T12:09:39.782068Z","iopub.execute_input":"2022-03-31T12:09:39.782382Z","iopub.status.idle":"2022-03-31T12:10:23.736457Z","shell.execute_reply.started":"2022-03-31T12:09:39.782346Z","shell.execute_reply":"2022-03-31T12:10:23.735687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"valid\"], batch_size=batch_size, collate_fn=data_collator\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n)","metadata":{"id":"uRAnW9D_Bxb3","execution":{"iopub.status.busy":"2022-03-31T12:10:23.737775Z","iopub.execute_input":"2022-03-31T12:10:23.73804Z","iopub.status.idle":"2022-03-31T12:10:23.744864Z","shell.execute_reply.started":"2022-03-31T12:10:23.738005Z","shell.execute_reply":"2022-03-31T12:10:23.74221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=EPOCH_NUMBER * len(train_dataloader))","metadata":{"id":"1aRJd3iuB5ao","outputId":"faa58bd7-ae9a-45e0-8ac8-d8933d2393dd","execution":{"iopub.status.busy":"2022-03-31T12:10:23.746583Z","iopub.execute_input":"2022-03-31T12:10:23.746831Z","iopub.status.idle":"2022-03-31T12:10:46.000278Z","shell.execute_reply.started":"2022-03-31T12:10:23.746797Z","shell.execute_reply":"2022-03-31T12:10:45.998398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start=time.time()\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc = train(model,train_dataloader,optimizer,None,train_pretrain=True)\n    valid_loss,valid_acc = evaluate(model,eval_dataloader,None,train_pretrain=True)\n    \n    results[\"BERT\"][\"Train Loss\"].append(train_loss.cpu().detach().numpy())\n    results[\"BERT\"][\"Train Acc\"].append(train_acc*100)\n    results[\"BERT\"][\"Val. Loss\"].append(valid_loss.cpu().detach().numpy())\n    results[\"BERT\"][\"Val. Acc\"].append(valid_acc*100)\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print()\n\ntest_loss,test_acc = evaluate(model,test_dataloader,None,train_pretrain=True)\nprint(f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc*100:.2f}%')\nstop = time.time()\nprint(f\"\\t Training time: {stop - start}s\")\nresults[\"BERT\"][\"Training time\"] = stop - start\nresults[\"BERT\"][\"test. loss\"] = test_loss\nresults[\"BERT\"][\"test. Acc\"] = test_acc","metadata":{"id":"tqUoVSEZNXFC","outputId":"9e19bd89-b699-4b6e-a0ec-3c09925972d0","execution":{"iopub.status.busy":"2022-03-31T12:10:46.001573Z","iopub.execute_input":"2022-03-31T12:10:46.001922Z","iopub.status.idle":"2022-03-31T12:48:02.20069Z","shell.execute_reply.started":"2022-03-31T12:10:46.001867Z","shell.execute_reply":"2022-03-31T12:48:02.199966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results('BERT')","metadata":{"id":"4Hhi4feSjANt","outputId":"a2877998-f650-42f2-bfe9-069fa38a08ce","execution":{"iopub.status.busy":"2022-03-31T12:48:02.202398Z","iopub.execute_input":"2022-03-31T12:48:02.203178Z","iopub.status.idle":"2022-03-31T12:48:02.600495Z","shell.execute_reply.started":"2022-03-31T12:48:02.203136Z","shell.execute_reply":"2022-03-31T12:48:02.599815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, we have a model that was trained on an epochs top. We see that two was enough. It obtains good performances compared to LSTM but is worse than GLOVE. ","metadata":{"id":"ZTd3qFQeUxuC"}},{"cell_type":"code","source":"with open('results_imdb.pkl', 'wb') as f:\n    pickle.dump(results, f)        \n","metadata":{"id":"rXkzpHhb4vqD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('results_imdb.pkl', 'rb') as f:\n    results = pickle.load(f)","metadata":{"id":"9pjDFrIfMJ81","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{"id":"ul1JXVdeMkhx"}},{"cell_type":"markdown","source":"Overall we notice that it was not necessary to use complex torp models for this type of stains, we only gain a few percentages of accuracy but we lose a lot of calculation time.\n","metadata":{"id":"nbvxP0RijRZv"}},{"cell_type":"code","source":"time_and_test = np.zeros((4,3))\nfor i in range(4):\n  for idx,j in enumerate([\"Training time\",'test. Acc','test. loss']):\n    time_and_test[i,idx]=results[models_name[i]][j]\ntime_and_test = pd.DataFrame(time_and_test,columns = [\"Training time\",'test. Acc','test. loss'], index = models_name)\ntime_and_test","metadata":{"id":"DigdjkzUO-rM","outputId":"bb5f0b65-8e44-4e98-9c28-46a00193f910","execution":{"iopub.status.busy":"2022-03-31T12:50:04.710862Z","iopub.execute_input":"2022-03-31T12:50:04.711509Z","iopub.status.idle":"2022-03-31T12:50:04.72304Z","shell.execute_reply.started":"2022-03-31T12:50:04.711474Z","shell.execute_reply":"2022-03-31T12:50:04.722323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}